Awesome — here’s a **single, self-contained FastAPI endpoint** you can paste into `backend/app/main.py`. It bundles a big demo context, does smart truncation in one function, sends the prompt to **Claude**, and returns the summary back to your popup.

Before you paste:

```
pip install anthropic
export ANTHROPIC_API_KEY=sk-...
```

Then add these imports near the top of `main.py` (alongside your other imports):

```python
import os, anthropic
from fastapi import Body, HTTPException
```

Now paste this **one long endpoint** anywhere after `app = FastAPI()`:

```python
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
_claude = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

@app.post("/api/llm/ask")
async def llm_ask(payload: dict = Body(...)):
    """
    ONE function that:
    - Builds a large demo context (requirements, eligible courses, samples, etc.)
    - Truncates intelligently to a total character budget
    - Calls Claude (Sonnet) for a concise summary + 3 bullets of suggestions
    - Returns { ok: true, answer, budget_info }
    Accepts: { query: str }   (optional: { query, extra_context })
    """

    if not ANTHROPIC_API_KEY:
        raise HTTPException(status_code=500, detail="ANTHROPIC_API_KEY not set")

    # ----- 0) Read user input -----
    query = (payload.get("query") or "").strip()
    extra_context = (payload.get("extra_context") or "").strip()

    # ----- 1) Big demo context (everything in one place) -----
    INSTRUCTIONS = """
You are Roary, an FIU course-planning assistant. Be brief, helpful, and actionable.
- If asked about schedules, prerequisites, or next courses, reason from context first.
- Prefer major requirements before electives.
- Keep responses to a short paragraph + 3 bullets max.
"""

    DEGREE_RULES = """
Major: Computer Science (BS) — SAMPLE RULES
- Recommend at most 3 core CS courses for the next term; leave room for electives.
- Respect prerequisites (listed below).
- If multiple paths exist, pick the most typical CS progression.
"""

    # Minimal but realistic prereq/eligibility sample (you can expand)
    CATALOG = """
COURSES & PREREQS (SAMPLE)
- COP 2210 Programming I (3 cr) — no prereq
- COP 3337 Programming II (3 cr) — prereq: COP 2210
- COP 3530 Data Structures (3 cr) — prereq: COP 3337
- COP 4338 Systems Programming (3 cr) — prereq: COP 3530
- COP 4610 Operating Systems (3 cr) — prereq: COP 4338
- COT 3100 Discrete Structures (3 cr) — prereq: MAC 2311
- MAC 2311 Calculus I (4 cr) — no prereq
- MAC 2312 Calculus II (4 cr) — prereq: MAC 2311
- STA 3033 Probability & Statistics (3 cr) — prereq: MAC 2311
- COP 4710 Database Management (3 cr) — prereq: COP 3530
- CEN 4010 Software Engineering I (3 cr) — prereq: COP 3530
- CAP 4104 Human-Computer Interaction (3 cr) — prereq: COP 3530
"""

    ELIGIBLE_SAMPLE = """
ELIGIBLE (SAMPLE for a mid-program student)
- COP 3530 Data Structures
- COT 3100 Discrete Structures
- STA 3033 Probability & Statistics
- COP 4710 Database Management
- CEN 4010 Software Engineering I
"""

    SCHEDULING_HINTS = """
Scheduling preferences (SAMPLE):
- Target 9 credits
- Morning classes preferred
- Avoid Friday if possible
"""

    # If caller sent extra context, include it
    USER_EXTRA = f"EXTRA CONTEXT FROM CALLER:\n{extra_context}\n" if extra_context else ""

    # ----- 2) Single-place truncation helpers -----
    def _truncate(s: str, max_chars: int) -> str:
        s = s or ""
        return s if len(s) <= max_chars else (s[: max(0, max_chars - 15)] + " …[truncated]")

    def _fit_budget(chunks, max_total: int):
        """
        Proportionally truncate all chunks so total <= max_total.
        Keeps a minimum floor for each chunk to avoid wiping a section entirely.
        Returns (trimmed_chunks, stats)
        """
        chunks = [c or "" for c in chunks]
        sep = "\n\n"
        sep_overhead = len(sep) * (len(chunks) - 1)
        sizes = [len(c) for c in chunks]
        total = sum(sizes) + sep_overhead

        if total <= max_total:
            return chunks, {"total": total, "max_total": max_total, "ratio": 1.0, "truncated": False}

        # available for content after accounting for separators
        content_budget = max(0, max_total - sep_overhead)
        sum_sizes = sum(sizes) or 1
        ratio = content_budget / sum_sizes

        # floor to keep some of each section
        MIN_SECT = 400
        trimmed = []
        for c, n in zip(chunks, sizes):
            target = max(MIN_SECT, int(n * ratio))
            trimmed.append(_truncate(c, target))

        # Final join length check (safety pass)
        final_total = sum(len(x) for x in trimmed) + sep_overhead
        if final_total > max_total:
            # a tighter pass: shrink each by same delta until we fit
            over = final_total - max_total
            step = max(1, over // len(trimmed) + 1)
            new_trimmed = []
            for t in trimmed:
                new_trimmed.append(_truncate(t, max(0, len(t) - step)))
            trimmed = new_trimmed

        return trimmed, {"total": final_total, "max_total": max_total, "ratio": ratio, "truncated": True}

    # ----- 3) Build the mega-prompt and truncate in ONE place -----
    # Order matters: instructions first, then degree rules, etc.
    blocks = [
        "INSTRUCTIONS:\n" + INSTRUCTIONS.strip(),
        "DEGREE RULES:\n" + DEGREE_RULES.strip(),
        "COURSE CATALOG:\n" + CATALOG.strip(),
        "ELIGIBLE SAMPLE:\n" + ELIGIBLE_SAMPLE.strip(),
        "SCHEDULING HINTS:\n" + SCHEDULING_HINTS.strip(),
        USER_EXTRA.strip(),
        "USER QUESTION:\n" + (query or "(no question provided)")
    ]

    # Total prompt budget (characters). Adjust as you like.
    PROMPT_BUDGET = 10_000
    trimmed_blocks, budget_info = _fit_budget(blocks, PROMPT_BUDGET)
    mega_prompt = "\n\n".join([b for b in trimmed_blocks if b])

    # ----- 4) Call Claude with a constrained task -----
    system_msg = "You are Roary, an FIU course advisor. Be concise and actionable."

    user_msg = (
        f"{mega_prompt}\n\n"
        "TASK:\n"
        "Return:\n"
        "1) A 2–4 sentence summary of the best next steps for the student.\n"
        "2) Exactly three bullets, each in the form: COURSE_CODE — one short reason.\n"
        "Keep it tight and concrete."
    )

    try:
        resp = _claude.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=400,
            temperature=0.2,
            system=system_msg,
            messages=[{"role": "user", "content": user_msg}],
        )
        parts = resp.content or []
        text = "\n".join(getattr(p, "text", "") for p in parts).strip() or "(no text)"
        return {"ok": True, "answer": text, "budget_info": budget_info}
    except anthropic.APIError as e:
        raise HTTPException(status_code=502, detail=f"Claude API error: {e}")
```

### How to use from your existing popup

You don’t need to change your listener logic — just point your fetch at the new route:

```js
const res = await fetch(`${API_BASE_URL}/llm/ask`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ query: userText }) // optional: extra_context
});
```

That’s it. This one route:

* Collects all context in one place
* Truncates it to a fixed budget
* Calls Claude
* Returns the short summary + 3 bullets you can display in your existing `<pre id="ai-output">`.
